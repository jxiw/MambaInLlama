model_name: meta-llama/Meta-Llama-3-8B-Instruct
ssm_layers: [0, 2, 4, 6, 8, 10, 12]
kl_weight: 0.1
ce_weight: 1
do_eval: false
train_datasets_path: [/data/junxiong/llama3_openhermes/]
prev_checkpoint_path: /data/junxiong/distill/llama3_6_mamba2/
output_dir: /data/junxiong/distill/llama3_7_mamba2/
seed: 42
save_steps: 5000
warmup_steps: 1500
per_device_train_batch_size: 1
per_device_eval_batch_size: 4
num_train_epochs: 1
gradient_accumulation_steps: 8
lr_scheduler_type: cosine
learning_rate: 8.0e-5
max_grad_norm: 0.1
progressive_step: 7
total_progressive_step: 16